{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling and Timing Code   <a href=\"https://colab.research.google.com/github/Ahmad-Zaki/Python-Notes/blob/main/Timing%20and%20Profiling%20Python%20code/timing-and-profiling.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you Create a function to do some task, there are often different implementations you can use to get the task done. Therefore, it can be useful sometimes to dig into the performance of each implementation to see how it performs or if it can be optimized in any way. Sometimes it's useful to check the execution time of a given command or set of commands; other times it's useful to measure a multiline process and determine where the bottleneck lies in a series of operations. \n",
    "\n",
    "IPython provides different commands for timing and profiling your code. We'll go over the following IPython magic commands:\n",
    "\n",
    "- ``%time``: Time the execution of a single statement\n",
    "- ``%timeit``: Time repeated execution of a single statement for more accuracy\n",
    "- ``%prun``: Run code with the profiler\n",
    "- ``%lprun``: Run code with the line-by-line profiler\n",
    "- ``%memit``: Measure the memory use of a single statement\n",
    "- ``%mprun``: Run code with the line-by-line memory profiler\n",
    "\n",
    "The last three commands are not bundled with IPython–you'll need to get the ``line_profiler`` and ``memory_profiler`` extensions, which we will discuss in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``%timeit`` line-magic and ``%%timeit`` cell-magic  to measure the average time it takes to execute a snippet of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32 µs ± 8.31 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tells us that the execution of this line takes in average $1.31 \\mu s$ with a standard deviation of $11.2 ns$. These statistics are calculated by measuring the time it takes to run this line of code 1 million times, repeated 7 times.\n",
    "\n",
    "Note that because this line takes very little time to execute, ``%timeit`` automatically does a large number of repetitions.\n",
    "For slower commands, ``%timeit`` will automatically adjust and perform fewer repetitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        total += i * (-1) ** j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the code block takes relatively longer time to execute, ``%timeit`` had adjusted to measure the time of only a single run, repeated 7 times.\n",
    "\n",
    "If we want to fix the number of runs or loops, we can set the number of loops (``-n``) and/or runs (``-r``) as flags for ``%timeit``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.4 ms ± 1.31 ms per loop (mean ± std. dev. of 2 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Set the number of runs to 2 (-r2).\n",
    "# Set the number of loops to 10 (-n10).\n",
    "\n",
    "%timeit -r2 -n10 [i for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can save the result of each run in a variable using the flag (``-o``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 µs ± 4.36 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "[0.00018676577999999892, 0.0001876240100000018, 0.00018658344000000113, 0.00019165486999999927, 0.00019916715000000237, 0.00018653177999999854, 0.0001868585299999978]\n",
      "Best time: 0.00018653177999999854\n",
      "Worst time: 0.00019916715000000237\n"
     ]
    }
   ],
   "source": [
    "times = %timeit -o a = sum(range(10000))\n",
    "print(times.timings)\n",
    "print(f\"Best time: {times.best}\")\n",
    "print(f\"Worst time: {times.worst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the other available options for ``%timeit``, you can IPython help functionality (i.e., type ``%timeit?`` at the IPython prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes repeating an operation is not the best option.\n",
    "For example, if we have a list that we'd like to sort, we might be misled by a repeated operation.\n",
    "Sorting a pre-sorted list is much faster than sorting an unsorted list, so the repetition will skew the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 ms ± 291 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "L = [random.random() for i in range(100000)]\n",
    "%timeit L.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, the ``%time`` magic function may be a better choice. It also is a good choice for longer-running commands, when short, system-related delays are unlikely to affect the result.\n",
    "Let's time the sorting of an unsorted and a presorted list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting an unsorted list:\n",
      "Wall time: 22 ms\n",
      "====================\n",
      "sorting an already sorted list:\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "L = [random.random() for i in range(100000)]\n",
    "print(\"sorting an unsorted list:\")\n",
    "%time L.sort()\n",
    "print(\"=\"*20)\n",
    "print(\"sorting an already sorted list:\")\n",
    "%time L.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that ``%time`` takes much longer time versus ``%timeit``, even for the presorted list. This is because ``%timeit`` will use the best available timing function on your system. Also, it prevents python from calling the garbage collector so that it doesn't interfere with the timing. another major difference is that ``%timeit`` makes it trivial to time the execution of the function for thousands of iterations (which is the only time where a timing result is meaningful). This decreases the importance of a single run taking longer than the others (e.g. due to your system resources being hogged by some other program).\n",
    "\n",
    "Just like ``%%timeit``, we can use ``%%time`` to time multiline scripts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 560 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        total += i * (-1) ** j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more information about ``%time``, you can IPython help functionality (i.e., type ``%time?`` at the IPython prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Code Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is more helpful to time commands in context instead of timing them on their own, Python contains a built-in code profiler (which you can read about in the Python documentation), but IPython offers a much more convenient way to use this profiler, in the form of the magic function ``%prun``.\n",
    "\n",
    "Lets define a function that does some calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_lists(N):\n",
    "    total = 0\n",
    "    for i in range(5):\n",
    "        L = [j ^ (j >> i) for j in range(N)]\n",
    "        total += sum(L)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call ``%prun`` with a function call to see the profiled results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         14 function calls in 1.084 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.905    0.181    0.905    0.181 3519952779.py:4(<listcomp>)\n",
      "        5    0.128    0.026    0.128    0.026 {built-in method builtins.sum}\n",
      "        1    0.040    0.040    1.073    1.073 3519952779.py:1(sum_of_lists)\n",
      "        1    0.011    0.011    1.084    1.084 <string>:1(<module>)\n",
      "        1    0.000    0.000    1.084    1.084 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
     ]
    }
   ],
   "source": [
    "%prun sum_of_lists(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a table that indicates, in order of total time on each function call, where the execution is spending the most time. In this case, the bulk of execution time is in the list comprehension inside ``sum_of_lists``.\n",
    "From here, we could start thinking about what changes we might make to improve the performance in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line by Line Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling of ``%prun`` is very handy, but it would be more convenient if we can get a line-by-line profiling report. Unfortunately, this isn't built into Python or IPython. However, we can still get it by installing the ``line_profiler`` package using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: line_profiler in c:\\users\\destr\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: IPython>=0.13 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from line_profiler) (7.29.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (3.0.20)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (5.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (58.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (0.4.4)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (0.18.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (0.1.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (5.1.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\destr\\anaconda3\\lib\\site-packages (from IPython>=0.13->line_profiler) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\destr\\anaconda3\\lib\\site-packages (from jedi>=0.16->IPython>=0.13->line_profiler) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\destr\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython>=0.13->line_profiler) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the ``line_profiler`` IPython extention provided in this package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ``%lprun`` command will do a line-by-line profiling of any function–in this case, we need to tell it explicitly which functions we're interested in profiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 1.96307 s\n",
      "File: C:\\Users\\DesTr\\AppData\\Local\\Temp/ipykernel_18004/3519952779.py\n",
      "Function: sum_of_lists at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def sum_of_lists(N):\n",
      "     2         1         23.0     23.0      0.0      total = 0\n",
      "     3         6        284.0     47.3      0.0      for i in range(5):\n",
      "     4         5   18464399.0 3692879.8     94.1          L = [j ^ (j >> i) for j in range(N)]\n",
      "     5         5    1165958.0 233191.6      5.9          total += sum(L)\n",
      "     6         1          7.0      7.0      0.0      return total"
     ]
    }
   ],
   "source": [
    "%lprun -f sum_of_lists sum_of_lists(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line gives us the key to reading the values in the table: the time is reported in $0.1\\mu s$. The total execution time is $1.98s$. We can see how many times each line is executed, its total runtime, and its percentage of the total time. Now we can know exactly what line takes the most time to execute, and decide what to change to improve the timing of the code.\n",
    "\n",
    "Although code profilers like ``%prun`` and ``%lprun`` provide very useful information, they measure the execution time only once. this may deliver inconsistent results depending the condition of your system when you run them. On the contrary, ``%timeit`` performs multible runs to get a more robust result for the time it takes to execute a statement or a script, but it is more time consuming for larger scripts with complex statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on ``%prun`` or ``%lprun``, as well as its available options, use the IPython help functionality (i.e., type ``%lprun?`` at the IPython prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of measuring our code performance is the amount of memory it takes during runtime. Just like ``line_profiler``, we can install ``memory_profiler`` using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\destr\\anaconda3\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\destr\\anaconda3\\lib\\site-packages (from memory_profiler) (5.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use IPython to load the extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory profiler extension contains two useful magic functions: the ``%memit`` magic (which offers a memory-measuring equivalent of ``%timeit``) and the ``%mprun`` function (which offers a memory-measuring equivalent of ``%lprun``).\n",
    "\n",
    "The ``%memit`` function can be used rather simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 161.12 MiB, increment: 67.38 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit sum_of_lists(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this function uses about 161 MB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a line-by-line description of memory use, we can use the ``%mprun`` magic.\n",
    "Unfortunately, this magic works only for functions defined in separate modules rather than the notebook itself, so we'll start by using the ``%%file`` magic to create a simple module called ``mprun_demo.py``, which contains our ``sum_of_lists`` function, with one addition that will make our memory profiling results more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mprun_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file mprun_demo.py\n",
    "def sum_of_lists(N):\n",
    "    total = 0\n",
    "    for i in range(5):\n",
    "        L = [j ^ (j >> i) for j in range(N)]\n",
    "        total += sum(L)\n",
    "        del L # remove reference to L\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the new version of this function and run the memory line profiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: d:\\Workspace\\Timing and Profiling Python code\\mprun_demo.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     1     94.1 MiB     94.1 MiB           1   def sum_of_lists(N):\n",
      "     2     94.1 MiB      0.0 MiB           1       total = 0\n",
      "     3     95.8 MiB     -1.0 MiB           6       for i in range(5):\n",
      "     4    112.6 MiB -14877566.9 MiB     2500015           L = [j ^ (j >> i) for j in range(N)]\n",
      "     5    112.6 MiB     -0.0 MiB           5           total += sum(L)\n",
      "     6     95.8 MiB    -72.2 MiB           5           del L # remove reference to L\n",
      "     7     95.5 MiB     -0.3 MiB           1       return total"
     ]
    }
   ],
   "source": [
    "from mprun_demo import sum_of_lists\n",
    "%mprun -f sum_of_lists sum_of_lists(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Increment column tells us how much each line affects the total memory budget: observe that when we create and delete the list ``L``, we are adding and removing about 25 MB of memory usage.\n",
    "This is on top of the background memory usage from the Python interpreter itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on ``%memit`` and ``%mprun``, as well as their available options, use the IPython help functionality (i.e., type ``%memit?`` at the IPython prompt)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec26fe652b81aa16e73f5f2489daa0bd9f355124f013eb8138d239f76af89ec7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
